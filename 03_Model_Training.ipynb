{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "b3b4su252bm4q4itx6ik",
   "authorId": "6149508575120",
   "authorName": "RPEGU",
   "authorEmail": "ranjeeta.pegu@snowflake.com",
   "sessionId": "158b546c-c0a5-4730-aec4-a8fd5781a4d4",
   "lastEditTime": 1747410312626
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49ed3e5-85ab-4bc3-97cd-4e2f4b8962ce",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "## Notebook Index\n1. [Feature Store ](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2201_FeatureStore_Creation%22)\n2. [Feature Reduction ](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2202_Feature_Reduction%22)\n3. Model Training ðŸ‘ˆ\n4. [Model Inference & Schdeuling](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2204_Batch_Inferencing%22)\n\n\n## Notebook Overview?\n\nIn this notebook, we will train machine learning models on a reduced dataset. The reduction process involves removing features that are either highly correlated or exhibit very low variance (less than 0.1).\n\nWe will then perform the following preprocessing steps:\n* Convert categorical variables into numerical format using one-hot encoding\n* Apply Min-Max scaling to standardize numerical features\n* After preprocessing, we will experiment with several modeling techniques, including:\n    * XGBoost \n    * LightGBM\n    * Random Forest\n* Model tuning will be performed using GridSearch, and all training will be executed using Snowflake ML.\n"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Libraries"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# Snowpark ML\nimport snowflake.snowpark.functions as F\nfrom snowflake.ml.modeling.pipeline import Pipeline \nfrom snowflake.ml.modeling.preprocessing import MinMaxScaler , OneHotEncoder\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.lightgbm import LGBMClassifier\n\nfrom snowflake.ml.modeling.ensemble import RandomForestClassifier\nfrom snowflake.ml.modeling.model_selection import GridSearchCV, RandomizedSearchCV\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\n\n# snowpark ML metrics\nfrom snowflake.ml.modeling.metrics import accuracy_score,f1_score,precision_score,roc_auc_score,roc_curve,recall_score\n\nfrom snowflake.snowpark.types import DecimalType, DoubleType, StringType\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.version import VERSION\nimport joblib\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "78065ce2-4f95-48b7-8ce8-ce3f6024531c",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "\n#database\ninput_database          = 'ML_MODELS'\nworking_database       = 'ML_MODELS'\n\n#schema\ninput_schema            = 'DS'\nworking_schema          = 'DS'\nfs_schema               = 'FEATURE_STORE'\nmodel_registry_schema   = 'ML_REGISTRY'\nstage_name = 'MODEL_OBJECT'\nstage = f\"@{working_database }.{working_schema }.{stage_name }\"\n\n\n\nwarehouse = 'DS_W'\nsnowpark_opt_warehouse  = 'SNOWPARK_OPT_WH'\nsession.use_warehouse(warehouse )\nsession.use_role('FR_SCIENTIST')\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('stage                        : {}'.format(stage))\n\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "Dataloader"
   },
   "source": "from snowflake.ml import dataset\n# Create a DataConnector from a Snowflake Dataset\nds = dataset.load_dataset(session, \"Reduced_Dataset\", \"V_1\")\n\n# Get a Snowpark DataFrame\ndf = ds.read.to_snowpark_dataframe()\n\n## get all columns with stringType= type\nexcluded = ['MEMBER_ID', 'TARGET','REF_MMYY','CAT_1','CAT_2','CAT_3','CAT_4','CAT_5']\nnum_cols = [col for col in df.columns if col not in excluded]\n\n\nexcluded = ['MEMBER_ID', 'TARGET','REF_MMYY']\n# Get string columns\nstring_columns = [field.name for field in df.schema.fields if isinstance(field.datatype, StringType)]\n\n# Filter out excluded columns\ncat_cols = [col for col in string_columns if col not in excluded]\n\ndef fix_values(columnn):\n    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\nfor col in cat_cols:\n    df = df.with_column(col, fix_values(col))\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "897b1f24-de25-498f-ab41-377ca3655102",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "#df.select('CAT_1','CAT_2','CAT_3','CAT_4','CAT_5')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "212a5f9b-ebd8-4b47-a797-566d0ac840fd",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "CREATE STAGE if not exists MODEL_OBJECT ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78049de1-12bc-40a0-8f58-5c005bf15c1f",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "preprocessing"
   },
   "source": "\npreprocessing_pipeline = Pipeline(\n    steps=[\n        \n        (\"OHE\",\n         OneHotEncoder(input_cols=cat_cols,\n                       output_cols=cat_cols,\n                       drop_input_cols=True,\n                       drop=\"first\",\n                       handle_unknown=\"ignore\",)\n         ),\n        (\"MMS\",MinMaxScaler(clip=True, \n                            input_cols=num_cols,\n                            output_cols=num_cols,))\n    ]\n\n)\n\n\njoblib.dump(preprocessing_pipeline, 'preprocessing_pipeline.joblib')\n#upload\nsession.file.put('preprocessing_pipeline.joblib',\n                 stage,auto_compress=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "71c1f26c-086c-4ba3-9dea-517f8cf9d960",
   "metadata": {
    "language": "python",
    "name": "download_Preprocessor"
   },
   "outputs": [],
   "source": "#download the preprocessor from the stage\n\n# Load the preprocessing pipeline object from stage- to do this, we download the preprocessing_pipeline.joblib.gz file to the warehouse\n# where our notebook is running, and then load it using joblib.\nsession.file.get(f'{stage}/preprocessing_pipeline.joblib', '/tmp')\nPIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib'\n\npreprocessing_pipeline = joblib.load(PIPELINE_FILE)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c42b3315-779a-4e3d-8976-4c1a8edcc70c",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "## Training and Validation Strategy\nFor model training, we will use data where **REF_MMYYYY = '042025'**, and for validation, we will use data with **REF_MMYYYY = '052025'**.The training dataset will be further split into training and test sets using an 80/20 random split to evaluate model performance"
  },
  {
   "cell_type": "code",
   "id": "19bacce5-f035-40c9-a278-a9532288ef41",
   "metadata": {
    "language": "python",
    "name": "getfeatures"
   },
   "outputs": [],
   "source": "## register this a udf\nlabel_col = 'TARGET'\ntrain_yrmo ='042025' # thid cwan be variable or parametrized\n\n\ndf = df.filter(F.col(\"REF_MMYY\") == train_yrmo).drop(\"REF_MMYY\")\n#train and test split\ntrain_df, test_df = df.random_split(weights = [0.80,0.20],seed=62)\n\ndef get_features(df, label_col:str):\n    return [col for col in df.columns if col not in ('MEMBER_ID', 'REF_MMYY',label_col)]\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d401e584-56ea-4644-9da6-fb5a6506ac8f",
   "metadata": {
    "language": "python",
    "name": "ApplyingPrep"
   },
   "outputs": [],
   "source": "session.use_warehouse(snowpark_opt_warehouse )\nFEATURE_COLS = get_features(train_df, label_col)\nprint(f\"Total featurs before preprocessing {len(FEATURE_COLS )} \")\n\n##applying pre-processor\ntrain_df=preprocessing_pipeline.fit(train_df).transform(train_df)\ntest_df=preprocessing_pipeline.transform(test_df)\n\nprint(f\"Total featuress before preprocessing {len(train_df.columns)} \")\nprint(f\"Total rows : {train_df.count()}\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18ee2482-9516-46d0-b256-aa4ff5594863",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "train_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78e00380-1c24-4fc3-8e00-9394c4f8b45d",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## XGBOOST CLASSIFIER"
  },
  {
   "cell_type": "code",
   "id": "8019fe1b-e886-4d61-b6d7-fd1632ae3ff9",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "FEATURE_COLS = get_features(train_df, label_col)\nOUTPUT_COLUMNS=\"PREDICTED_TARGET\"\nlabel_col='TARGET'\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d576da1d-de72-4189-93f8-2182f85c115c",
   "metadata": {
    "language": "python",
    "name": "xgboost_modeltrain"
   },
   "outputs": [],
   "source": "XGB_Classifier= XGBClassifier(\n    input_cols=FEATURE_COLS ,\n    label_cols=label_col,\n    output_cols=OUTPUT_COLUMNS\n)\n# Train\nXGB_Classifier.fit(train_df)\n\n#  evaluation \npredict_on_training_data = XGB_Classifier.predict(train_df)\n\ntraining_accuracy = accuracy_score(df=predict_on_training_data, \n                                   y_true_col_names=[\"TARGET\"],\n                                   y_pred_col_names=[\"PREDICTED_TARGET\"])\n\n\nresult = XGB_Classifier.predict(test_df)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3e9b32b-bb31-43b1-814f-509f08100057",
   "metadata": {
    "language": "python",
    "name": "xgboostMetrics"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.metrics import confusion_matrix\n\n\nmetrics = {\n\"accuracy\":accuracy_score(df=result, \n                          y_true_col_names=\"TARGET\", \n                          y_pred_col_names=\"PREDICTED_TARGET\"),\n\n\"precision\":precision_score(df=result,\n                            y_true_col_names=\"TARGET\", \n                            y_pred_col_names=\"PREDICTED_TARGET\"),\n\n\n\"recall\": recall_score(df=result, \n                       y_true_col_names=\"TARGET\",\n                       y_pred_col_names=\"PREDICTED_TARGET\"),\n\n\n\n\"f1_score\":f1_score(df=result,\n                   y_true_col_names=\"TARGET\",\n                   y_pred_col_names=\"PREDICTED_TARGET\"),\n\"confusion_matrix\":confusion_matrix(df=result, \n                                    y_true_col_name=\"TARGET\",\n                                    y_pred_col_name=\"PREDICTED_TARGET\").tolist()\n}\n\nprint(f\" The Score for the xgboost model :\\n {metrics}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d9a8672-b593-46c4-9f37-6440e9cceb09",
   "metadata": {
    "language": "python",
    "name": "xgb_mdlreg"
   },
   "outputs": [],
   "source": "# Get sample input data to pass into the registry logging function\nX = train_df.select(FEATURE_COLS).limit(100)\ndb = working_database \nschema =model_registry_schema \n\n# Create a registry and log the model\nreg = Registry(session=session, database_name=db, schema_name=schema)\n\n\nmodel_name = \"ML_XGBOOST_MODEL\"\nversion_name = \"v1\"\n\n# Let's first log the very first model we trained\nmv = reg.log_model(\n    model_name=model_name,\n    version_name=version_name,\n    model= XGB_Classifier,\n    metrics=metrics ,\n    sample_input_data=X, # to provide the feature schema\n)\n\n\n# Add a description\nmv.comment = \"\"\"This is the first iteration of our ml poc  model. \nIt is used for demo purposes and it is simple xgboost model.\"\"\"\n\n\n# Let's confirm they were added\nreg.get_model(model_name).show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e7c24973-418b-4ef2-9e93-054d51b123d1",
   "metadata": {
    "name": "rf",
    "collapsed": false
   },
   "source": "## RANDOM FOREST "
  },
  {
   "cell_type": "code",
   "id": "e51f834c-032b-4b74-a370-f6513f5891bb",
   "metadata": {
    "language": "python",
    "name": "randomForest"
   },
   "outputs": [],
   "source": "FEATURE_COLS = get_features(train_df, label_col)\nOUTPUT_COLUMNS=\"PREDICTED_TARGET\"\nlabel_col='TARGET'\n\n\nRandomForest= RandomForestClassifier(\n    input_cols=FEATURE_COLS ,\n    label_cols=label_col,\n    output_cols=OUTPUT_COLUMNS\n)\n# Train\nRandomForest.fit(train_df)\n\n#  evaluation \npredict_on_training_data = RandomForest.predict(train_df)\n\ntraining_accuracy = accuracy_score(df=predict_on_training_data, \n                                   y_true_col_names=[\"TARGET\"],\n                                   y_pred_col_names=[\"PREDICTED_TARGET\"])\n\n\nresult = RandomForest.predict(test_df)\n\n## metrics\n\nmetrics = {\n\"accuracy\":accuracy_score(df=result ,\n                          y_true_col_names=\"TARGET\", \n                          y_pred_col_names=\"PREDICTED_TARGET\"),\n\n\"precision\":precision_score(df=result,\n                            y_true_col_names=\"TARGET\", \n                            y_pred_col_names=\"PREDICTED_TARGET\"),\n\n\n\"recall\": recall_score(df=result, \n                       y_true_col_names=\"TARGET\",\n                       y_pred_col_names=\"PREDICTED_TARGET\"),\n\n\n\n\"f1_score\":f1_score(df=result,\n                   y_true_col_names=\"TARGET\",\n                   y_pred_col_names=\"PREDICTED_TARGET\"),\n\"confusion_matrix\":confusion_matrix(df=result, \n                                    y_true_col_name=\"TARGET\",\n                                    y_pred_col_name=\"PREDICTED_TARGET\").tolist()\n}\n\nprint(f\" The Score for the xgboost model :\\n {metrics}\")\n\n\n## register the model \n\nmodel_name = \"ML_RANDOMFOREST_MODEL\"\nversion_name = \"v1\"\n\n# Let's first log the very first model we trained\nmv = reg.log_model(\n    model_name=model_name,\n    version_name=version_name,\n    model= RandomForest,\n    metrics=metrics ,\n    sample_input_data=X, # to provide the feature schema\n)\n\n\n# Add a description\nmv.comment = \"\"\"This is the first iteration of poc  random forest model. \nIt is used for demo purposes and it is simple random_forest model.\"\"\"\n\n\n# Let's confirm they were added\nreg.get_model(model_name).show_versions()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "129408cb-6d12-4fe4-8fbf-ed814a9c420a",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "## HYPERPARAMETER TUNING\n\nWe will perform a **grid search** using **Snowflake ML** to find the optimal hyperparameters for our model. Grid search systematically tests combinations of hyperparameter values to identify the configuration that yields the best model performance.\n\nThis process will be executed within Snowflake using its built-in machine learning capabilities, ensuring that all computation stays close to the data and leverages the scalability of the Snowflake platform.\n"
  },
  {
   "cell_type": "code",
   "id": "e4e6dd00-2e15-4e87-8b10-3c85b537223d",
   "metadata": {
    "language": "python",
    "name": "grid_search"
   },
   "outputs": [],
   "source": "## parameter grid \nFEATURE_COLS = get_features(train_df, label_col)\nOUTPUT_COLUMNS=\"PREDICTED_TARGET\"\nlabel_col='TARGET'\n\n\n\nparameters = {\n        \"n_estimators\": [100, 300, 500],\n        \"learning_rate\": [0.1, 0.3, 0.5],\n        \"max_depth\": list(range(3, 5, 1)),\n        \"min_child_weight\": list(range(3, 5, 1)),\n    }\n    \nn_folds = 5\n\nestimator = XGBClassifier()\n\nGridSearch_clf = GridSearchCV(estimator= estimator,\n                   param_grid=parameters ,\n                   cv = n_folds,\n                   input_cols=FEATURE_COLS ,\n                   label_cols=label_col,\n                   output_cols=OUTPUT_COLUMNS\n                   )\nGridSearch_clf.fit(train_df)\n\nresult = GridSearch_clf.predict(test_df )\nprint(GridSearch_clf.to_sklearn().best_estimator_)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0dcd743-c478-447d-988f-b7838318edd4",
   "metadata": {
    "language": "python",
    "name": "gs_mertics"
   },
   "outputs": [],
   "source": "metrics = {\n\"accuracy\":accuracy_score(df=result, \n                          y_true_col_names=\"TARGET\", \n                          y_pred_col_names=\"PREDICTED_TARGET\"),\n\n\"precision\":precision_score(df=result,\n                            y_true_col_names=\"TARGET\", \n                            y_pred_col_names=\"PREDICTED_TARGET\"),\n\n\n\"recall\": recall_score(df=result, \n                       y_true_col_names=\"TARGET\",\n                       y_pred_col_names=\"PREDICTED_TARGET\"),\n\n\n\n\"f1_score\":f1_score(df=result,\n                   y_true_col_names=\"TARGET\",\n                   y_pred_col_names=\"PREDICTED_TARGET\"),\n\"confusion_matrix\":confusion_matrix(df=result, \n                                    y_true_col_name=\"TARGET\",\n                                    y_pred_col_name=\"PREDICTED_TARGET\").tolist()\n}\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "112bad49-bcc8-461c-939c-e82c65481815",
   "metadata": {
    "language": "python",
    "name": "log_gridSearch"
   },
   "outputs": [],
   "source": "\n\n# Now, let's log the optimal model from GridSearchCV\nmodel_name = \"ML_GRIDSEARCH_POC_MODEL\"\nversion_name = \"v2\"\n\n# Let's first log the very second model we trained\nmv = reg.log_model(\n    model_name=model_name,\n    version_name=version_name,\n    model= XGB_Classifier,\n    metrics=metrics ,\n    sample_input_data=X, # to provide the feature schema\n)\n\n# Add evaluation metric\n#mv2.set_metric(metric_name=\"accuracy_score\", value=accuracy)\n\n# Add a description\nmv.comment = \"This is the first iteration of our POC model \\\n                        where we performed hyperparameter optimization with Grid Search.\"",
   "execution_count": null
  }
 ]
}