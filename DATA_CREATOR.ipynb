{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ahdsg3cgzn7hlrk5uceh",
   "authorId": "6149508575120",
   "authorName": "RPEGU",
   "authorEmail": "ranjeeta.pegu@snowflake.com",
   "sessionId": "39450d7a-fe3e-4414-8f90-d4cc1c076857",
   "lastEditTime": 1747411110799
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c9dad3-3bf7-4fa5-8d7e-c34bb0169621",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "## Notebook Index\n1. [Feature Store](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2201_FeatureStore_Creation%22)  \n2. [Feature Reduction](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2202_Feature_Reduction%22)  \n3. [Model Training](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2203_Model_Training%22)  \n4. [Model Inference & scheduling ](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2204_Batch_Inferencing%22)\n\n\n\n### What this notebook does\nThis notebook generates synthetic data for a demo use case. The dataset includes a member_id, a mix of numerical and categorical features, and a binary target variable.\n\nUsing Scikit-Learnâ€™s make_classification, we generate:\n* 150 features from make_classification (i.e., base features)\n* 200 low-variance features\n* 150 highly correlated features (based on the 150 base features)\n* 5 categorical columns\n* Missing data in selected columns\n* A binary target column"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nprint(f' Database Name :{session.get_current_database()}')\nprint(f' Schema Name :{session.get_current_schema()}')\nprint(f' warehouse Name :{session.get_current_warehouse()}')\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": true,
    "codeCollapsed": false
   },
   "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\n# Parameters\nn_samples = 100000\nbase_features = 150\nlow_variance_features = 200\ncorrelated_features = 150\ntotal_numeric_features = base_features + low_variance_features + correlated_features\n\n# Step 1: Generate base numerical features and target\nX, y = make_classification(\n    n_samples=n_samples,\n    n_features=base_features,\n    n_informative=60,\n    n_redundant=60,\n    n_repeated=0,\n    n_classes=2,\n    random_state=42,\n    shuffle=False\n)\n\n# Create DataFrame for base features\ndf = pd.DataFrame(X, columns=[f'FEATURE_{i}' for i in range(base_features)])\ndf['TARGET'] = y\n\n# Step 2: Add 200 Low-Variance Features\nfor i in range(1, low_variance_features + 1):\n    if i == 1:\n        df[f'FEATURE_LOW_VAR_{i}'] = 1  # Constant column\n    else:\n        df[f'FEATURE_LOW_VAR_{i}'] = np.random.choice([0, 1], size=n_samples, p=[0.98, 0.02])\n\n# Step 3: Add 150 Highly Correlated Features\nfor i in range(1, correlated_features + 1):\n    source_feature = f'FEATURE_{(i - 1) % base_features}'  # Cycle through base features\n    df[f'FEATURE_CORR_{i}'] = df[source_feature] * 0.95 + np.random.normal(0, 0.01, n_samples)\n\n# Step 4: Add 5 Specific Categorical Columns with realistic values\ndf['CAT_1'] = np.random.choice(['Male', 'Female'], size=n_samples)\ndf['CAT_2'] = np.random.choice(['online', 'retail'], size=n_samples)\ndf['CAT_3'] = np.random.choice(['tier_1', 'tier_2', 'tier_3'], size=n_samples)\ndf['CAT_4'] = np.random.choice(['credit', 'debit'], size=n_samples)\ndf['CAT_5'] = np.random.choice(['single', 'family'], size=n_samples)\n\n# Step 5: Add Missing Values\ndef add_missing_values(df, cols, fraction=0.05):\n    for col in cols:\n        missing_indices = df.sample(frac=fraction, random_state=42).index\n        df.loc[missing_indices, col] = np.nan\n    return df\n\n# Introduce missing values in numeric and categorical columns\nnumeric_missing = ['FEATURE_0', 'FEATURE_10', 'FEATURE_50', 'FEATURE_LOW_VAR_2', 'FEATURE_CORR_1']\ncategorical_missing = ['CAT_1', 'CAT_3', 'CAT_5']\n\ndf = add_missing_values(df, numeric_missing)\ndf = add_missing_values(df, categorical_missing)\n\n# Step 6: Add MEMBER_ID Column\ndf['MEMBER_ID'] = [f'member_{i}' for i in range(len(df))]\n\n# Step 7: Add REF_MMYY Column with random assignment of '042025' or '052025'\ndf['REF_MMYY'] = np.random.choice(['042025', '052025'], size=n_samples)\n\n# Final shape check\nprint(f\"Final Data Shape: {df.shape}\")  # Should be (100000, ~507)\n\n# Optional: Preview the data\n# print(df.head())\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d87a7d4e-e5f6-4192-9474-3304458b1db5",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "print(f\"Final Data Shape: {df.shape}\") ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "SplitDf"
   },
   "source": "import numpy as np\n\n# Identify the 500 numerical feature columns (excluding categorical, TARGET, MEMBER_ID, MMYYYY)\nnumeric_feature_cols = [col for col in df.columns \n                        if col.startswith('FEATURE_') or col.startswith('FEATURE_LOW_VAR_') or col.startswith('FEATURE_CORR_')]\n\n# Sanity check\nassert len(numeric_feature_cols) == 500, f\"Expected 500 numeric features, found {len(numeric_feature_cols)}\"\n\n# Split numeric features into 4 equal parts\nfeature_splits = np.array_split(numeric_feature_cols, 4)\n\n# Create four separate DataFrames with 125 features each + MEMBER_ID + REF_MMYY\ndf1 = df[list(feature_splits[0]) + ['MEMBER_ID', 'REF_MMYY','CAT_1','CAT_2']].copy()\ndf2 = df[list(feature_splits[1]) + ['MEMBER_ID', 'REF_MMYY','CAT_3']].copy()\ndf3 = df[list(feature_splits[2]) + ['MEMBER_ID', 'REF_MMYY','CAT_4']].copy()\ndf4 = df[list(feature_splits[3]) + ['MEMBER_ID', 'REF_MMYY','CAT_5']].copy()\n\n# Print shapes for verification\nprint(f\"df1 shape: {df1.shape}\")\nprint(f\"df2 shape: {df2.shape}\")\nprint(f\"df3 shape: {df3.shape}\")\nprint(f\"df4 shape: {df4.shape}\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "be3a9c65-dc6c-41ac-8362-643921917bf3",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "\n# Get remaining columns in df that are not in any of df1 to df4\nr_cols = ['MEMBER_ID', 'REF_MMYY','TARGET']\n\ndf_main = df[r_cols].copy()\nprint(f\"df_main shape: {df_main.shape}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78da1812-4a89-498b-aac2-24199045ada8",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "def write_table(session,df,snf_tbl):\n    sdf = session.create_dataframe(df)\n    sdf.write.mode(\"overwrite\").save_as_table(snf_tbl, table_type=\"transient\")\n\n\nwrite_table(session,df1,'DEMO_TBL_1')\nwrite_table(session,df2,'DEMO_TBL_2')  \nwrite_table(session,df3,'DEMO_TBL_3')\nwrite_table(session,df4,'DEMO_TBL_4')\nwrite_table(session,df_main,'DEMO_TARGETS_TBL')",
   "execution_count": null
  }
 ]
}