{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "3wyxcb5hievjtgrkuh62",
   "authorId": "6149508575120",
   "authorName": "RPEGU",
   "authorEmail": "ranjeeta.pegu@snowflake.com",
   "sessionId": "9cd805c0-602d-4bce-bca0-b8a90c25c3a5",
   "lastEditTime": 1747409778109
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9ef28c-6bc7-48f7-9712-8cb567380d94",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "## Notebook Index\n1. [Feature Store ](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2201_FeatureStore_Creation%22)\n2. [Feature Reduction ](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2202_Feature_Reduction%22)\n3. [Model Training ](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2203_Model_Training%22)\n4. Model Inference & scheduling ðŸ‘ˆ\n\n\n\n## Notebook Overview?\n\nIn this notebook, you'll learn how to perform batch inferencing using Snowflake ML by:\n* Leveraging features stored in the feature store\n* Loading the model signature from the Snowflake Model Registry\n* Running predictions at scale within Snowflake\n\nThis approach enables you to generate model predictions for large datasets efficiently, all within the Snowflake platform. Additionally, the notebook can be scheduled to run at regular intervals, allowing for fully automated and production-grade batch scoring workflows"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport pandas as pd\n\n# Snowpark ML\nimport snowflake.snowpark.functions as F\nfrom snowflake.ml.modeling.pipeline import Pipeline \n\n# Snowflake Feature Store\nfrom snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,  CreationMode)\n## Snwoflake Model Registry\nfrom snowflake.ml.registry import Registry\nimport joblib\n\nfrom snowflake.snowpark.types import DecimalType, DoubleType, StringType\nfrom snowflake.snowpark.version import VERSION\nimport joblib\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "source": "\n#database\ninput_database          = 'ML_MODELS'\nworking_database       = 'ML_MODELS'\n\n#schema\ninput_schema            = 'DS'\nworking_schema          = 'DS'\nfs_schema               = 'FEATURE_STORE'\nmodel_registry_schema   = 'ML_REGISTRY'\nstage_name = 'MODEL_OBJECT'\nstage = f\"@{working_database }.{working_schema }.{stage_name }\"\n\n\n\nwarehouse = 'DS_W'\nsnowpark_opt_warehouse  = 'SNOWPARK_OPT_WH'\nsession.use_warehouse(warehouse )\n\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('stage                        : {}'.format(stage))\n\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fb310ac4-abe1-47ad-8999-3107b48f7c99",
   "metadata": {
    "language": "python",
    "name": "Variables"
   },
   "outputs": [],
   "source": "\n\n\nmodel_name= 'ML_XGBOOST_MODEL'\nversion= 'V1'\nref_mmyyyy= '052025'\ndataset_version = f'V1_{ref_mmyyyy}'\nprint(dataset_version)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "46ccb5d7-4b3b-419e-97fd-52c6100d8060",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "Connecting the Snowflake Feature Store, to retrive the features "
  },
  {
   "cell_type": "code",
   "id": "a123ce7d-e3db-4f30-90b9-d5a9f739d312",
   "metadata": {
    "language": "python",
    "name": "Fs_conn"
   },
   "outputs": [],
   "source": "try:\n    fs = FeatureStore(\n        session=session,\n        database=working_database,\n        name=fs_schema,\n        default_warehouse=warehouse,\n        creation_mode=CreationMode.FAIL_IF_NOT_EXISTS\n    )\nexcept:\n    fs = FeatureStore(\n        session=session,\n        database=working_database,\n        name=fs_schema,\n        default_warehouse=warehouse,\n        creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n    )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "## Retrieve the features \n\nfv_feature_ent1_instance  = fs.get_feature_view(\"FV_FEATURE_ENT_1\", \"V_1\")\nfv_feature_ent2_instance  = fs.get_feature_view(\"FV_FEATURE_ENT_2\", \"V_1\")\nfv_feature_ent3_instance  = fs.get_feature_view(\"FV_FEATURE_ENT_3\", \"V_1\")\nfv_feature_ent4_instance  = fs.get_feature_view(\"FV_FEATURE_ENT_4\", \"V_1\")\n\n\nfv_list = [fv_feature_ent1_instance, \n           fv_feature_ent2_instance, \n           fv_feature_ent3_instance,\n           fv_feature_ent4_instance] \n\n\nuniverse_tbl = '.'.join([input_database, input_schema, 'DEMO_TARGETS_TBL'])\nuniverse_sdf            = session.table(universe_tbl).filter(F.col(\"REF_MMYY\") == ref_mmyyyy)\n\n\n#get the input signature from the desired model from the model registr\n\nreg = Registry(session, database_name = working_database,schema_name = model_registry_schema)\nreg.show_models()\nmv = reg.get_model(model_name).version(\"v1\")\n# the input signature of model\ninput_signature = mv.show_functions()[0].get(\"signature\").inputs\ninput_cols = [c.name for c in input_signature]\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "be531708-02b3-44a4-8bf1-44ac4d78e4a5",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "--ALTER DATASET INFERENCE_DATASET drop version 'V1_052025'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5caedd9e-d6c1-4060-aff2-eb69b48d1fd7",
   "metadata": {
    "language": "python",
    "name": "InferenceDS"
   },
   "outputs": [],
   "source": "# Create feature view slices\n#ds_cols = []\n## Manually add the categorical columns as the column name modified in one-hote-encoding\nds_cols = []\nslice_list = []\n\ninput_cols=input_cols\n#input_cols\nfor fv in fv_list:\n    fv_cols = list(fv._feature_desc)\n    #fv_cols\n    slice_cols = [\n    col for col in fv_cols\n    if col not in ds_cols and (\n        col in input_cols or col in {\"CAT_1\", \"CAT_2\", \"CAT_3\", \"CAT_4\", \"CAT_5\"}\n    )]\n    \n    if len(slice_cols) > 0:\n        slice_list.append(fv.slice(slice_cols))\n        ds_cols += fv_cols\n        \ndataset = fs.generate_dataset(\n    name=f\"{working_database}.{working_schema}.INFERENCE_DATASET\",\n    spine_df=universe_sdf,\n    features = slice_list,\n    version=dataset_version,\n    #output_type=\"table\",\n    spine_label_cols=[\"TARGET\"],\n    desc=\"training dataset for ml demo\"\n)    \n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eac93d40-de8a-4688-9594-dda5a8684986",
   "metadata": {
    "language": "python",
    "name": "Ds2Df"
   },
   "outputs": [],
   "source": "from snowflake.ml import dataset\n# Create a DataConnector from a Snowflake Dataset\nds = dataset.load_dataset(session, \"INFERENCE_DATASET\", dataset_version)\n\n# Get a Snowpark DataFrame\ndf = ds.read.to_snowpark_dataframe()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad26ebdf-b90f-4bec-b2a3-72c6850f7977",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79984021-dda3-4d31-b3c4-dfa00ba68cb6",
   "metadata": {
    "language": "python",
    "name": "preprocessing"
   },
   "outputs": [],
   "source": "excluded = ['MEMBER', 'TARGET','REF_MMYY']\nfeatures = [col for col in df.columns if col not in excluded]\n\ncat_cols = [field.name for field in df.schema.fields if isinstance(field.datatype, StringType)]\ncat_cols=[col for col in cat_cols if col != excluded] \n#cat_cols\n## Making sure the string column has values in UPPER Case, no space  or special character\n\n# Apply transformations to upper and remove space\ndef fix_values(columnn):\n    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n\n\nfor col in cat_cols:\n    df = df.with_column(col, fix_values(col))\n\n\nnum_cols = [field.name for field in df.schema.fields if isinstance(field.datatype, DecimalType)]\nfor colname in num_cols: df = df.with_column(colname,sdf[colname].cast(DoubleType()))\n\n\n## CATEGORICAL COLUMNS\n# Create dictionary to fill nulls with 'UNKNOWN'\nfill_values_cat = {col_name: 'UNKNOWN' for col_name in cat_cols}\n\n## NUMERICAL COLUMNS\n# Create dictionary to fill nulls with 0.00\nfill_values_num = {col_name: 0.00 for col_name in num_cols}\n\n# Merge both dictionaries\nfill_values = {**fill_values_cat, **fill_values_num}\n\n# Apply fillna to the Snowpark DataFrame\ndf = df.fillna(fill_values)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d94d50c1-011a-40c3-b994-c9e4b973c1e8",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51a38280-0d4c-4160-bde1-2a8c59088122",
   "metadata": {
    "language": "python",
    "name": "Downloadpreprocessing"
   },
   "outputs": [],
   "source": "## get all columns with stringType= type\nexcluded = ['MEMBER_ID', 'TARGET','REF_MMYY','CAT_1','CAT_2','CAT_3','CAT_4','CAT_5']\nnum_cols = [col for col in df.columns if col not in excluded]\n\n\nexcluded = ['MEMBER_ID', 'TARGET','REF_MMYY']\n# Get string columns\nstring_columns = [field.name for field in df.schema.fields if isinstance(field.datatype, StringType)]\n\n# Filter out excluded columns\ncat_cols = [col for col in string_columns if col not in excluded]\n\n\n# Load the preprocessing pipeline object from stage- to do this, we download the preprocessing_pipeline.joblib.gz file to the warehouse\n# where our notebook is running, and then load it using joblib.\nsession.file.get(f'{stage}/preprocessing_pipeline.joblib', '/tmp')\nPIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib'\n\npreprocessing_pipeline = joblib.load(PIPELINE_FILE)\n\ndf=preprocessing_pipeline.fit(df).transform(df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b39c9811-cd67-457f-a130-c02d608f1e6f",
   "metadata": {
    "language": "python",
    "name": "Prediction"
   },
   "outputs": [],
   "source": "prediction_result = mv.run(df, function_name =\"PREDICT\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "990cbf5f-453e-4887-bb25-96ae6401feea",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "result = prediction_result.select('MEMBER_ID','PREDICTED_TARGET','REF_MMYY')\nresult.show()\nresult.write.mode(\"append\").save_as_table(\"TARGET_CUSTOMER_PREDICTION\", table_type=\"transient\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8eec90dd-dd87-463b-b293-05b30dbbbd71",
   "metadata": {
    "language": "sql",
    "name": "sqlresult"
   },
   "outputs": [],
   "source": "Select * from TARGET_CUSTOMER_PREDICTION",
   "execution_count": null
  }
 ]
}