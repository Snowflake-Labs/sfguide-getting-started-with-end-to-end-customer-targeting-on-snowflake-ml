{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "qtepo4ncvazxrejwjafk",
   "authorId": "6149508575120",
   "authorName": "RPEGU",
   "authorEmail": "ranjeeta.pegu@snowflake.com",
   "sessionId": "02a66413-f8b4-414e-bf63-880d8efc6307",
   "lastEditTime": 1747410200422
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e11803-5573-4dd8-b8b1-9abb0fb8b5fc",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Notebook Index\n1. [Feature Store](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2201_FeatureStore_Creation%22) \n2. Feature Reduction ðŸ‘ˆ\n3. [Model Training](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2203_Model_Training%22)\n4. [Model Inference & scheduling ](https://app.snowflake.com/sfpscogs/rpegu_aiml/#/notebooks/ML_MODELS.DS.%2204_Batch_Inferencing%22)\n\n\n## What this notebook does?\nIn this notebook we create the Dataset that will be used for modeling purpose, we also do feature reduction and remove the unwanted or redundant features before doing the model training\n\n#### Feature Reduction\nFeature Reduction Using Correlation and Variance Threshold\n\nFeature reduction helps simplify models, reduce overfitting, and improve computational efficiency.\n\nVariance Threshold: Removes features with low variance, which carry little information for prediction. For example, if a featureâ€™s value is almost constant across samples, it contributes minimally to model learning.\n\nCorrelation Analysis: Identifies highly correlated features (multicollinearity). Strongly correlated features (e.g., correlation > 0.8) provide redundant information. Retaining only one from such groups helps prevent model instability and reduces complexity.\n\nBest Practice: Apply variance thresholding first to remove uninformative features. Then perform correlation analysis to eliminate redundancy among remaining features."
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Context"
   },
   "source": "# Import python packages\nimport pandas as pd\n\n# Snowflake Feature Store\nfrom snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,  CreationMode)\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.types import DecimalType, DoubleType, StringType\nfrom snowflake.snowpark.functions import month,year,col,sum,when,upper, regexp_replace\n\nfrom snowflake.snowpark.version import VERSION\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c7724814-5d55-44d1-9304-95857f059659",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "\n#database\ninput_database          = 'ML_MODELS'\nworking_database       = 'ML_MODELS'\n\n#schema\ninput_schema            = 'DS'\nworking_schema          = 'DS'\nfs_schema               = 'FEATURE_STORE'\n\nwarehouse = 'DS_W'\nsnowpark_opt_warehouse  = 'SNOWPARK_OPT_WH'\nsession.use_warehouse(warehouse )\nsession.use_role('FR_SCIENTIST')\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "source": "tablenm =f\"{working_database}.{working_schema}.DEMO_DATASET_V_1\"\ndf = session.table(tablenm)\nprint(f\"total columns in the dataframe :{len(df.columns)}\")\nprint(f\"total rows in the dataframe :{df.count()}\")\ndf.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5f495711-ef0f-4085-a10e-9687059badec",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "### Pre-Processing Steps\nFor the demo purposes, I will the REF_MMYY=042025 for Model Training  and REF_MMYY=052025 for validation for purposes.\n\n**Data Type Conversion**:\n* Convert all columns with DECIMAL data type to DOUBLE for consistency in numerical calculations.\n\n**String Column Standardization**:\n\n* Convert all string values to UPPERCASE.\n* Remove spaces and special characters to ensure clean and consistent categorical values."
  },
  {
   "cell_type": "code",
   "id": "e556b521-8140-469d-bdd1-840d6faea182",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "source": "\n#sdf = df\n\n\n## get all columns with stringType= type\nexcluded = ['MEMBER', 'TARGET','REF_MMYY']\nfeatures = [col for col in df.columns if col not in excluded]\n\ncat_cols = [field.name for field in df.schema.fields if isinstance(field.datatype, StringType)]\ncat_cols=[col for col in cat_cols if col != excluded] \ncat_cols\n## Making sure the string column has values in UPPER Case, no space  or special character\n\n# Apply transformations to upper and remove space\ndef fix_values(columnn):\n    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n\n\nfor col in cat_cols:\n    sdf = df.with_column(col, fix_values(col))\n\n\n##NUMERICALCOL\n## converting the decimal into double\n\ndecimal_col = [field.name for field in sdf.schema.fields if isinstance(field.datatype, DecimalType)]\nfor colname in decimal_col: sdf = sdf.with_column(colname,sdf[colname].cast(DoubleType()))\n\n ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bc1ecf2a-25ca-49b2-a992-ceac87bb5d8b",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "sdf.show()\n###sdf.columns",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "711c06ea-1c62-45b5-ad43-eebe8e48cf33",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "### Missing Value Handling:\n\n* Apply KNN Imputer for missing values in numeric columns.\n\n* For string columns, handle nulls by either imputing with a default value or marking them explicitly as 'UNKNOWN'."
  },
  {
   "cell_type": "code",
   "id": "c77e1c08-d697-465e-a45b-16424c06c6b0",
   "metadata": {
    "language": "python",
    "name": "MissingDataHandling"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.impute import KNNImputer\nMissing_data_col=['FEATURE_0', 'FEATURE_10', 'FEATURE_50']\n#sdf[Missing_data_col].show()\n## using the KNN imputer natively inside Snowflake Ml to handle missing Data \n\n# Initialize KNNImputer\nimputer = KNNImputer(input_cols=Missing_data_col,\n                     output_cols=Missing_data_col,\n                     n_neighbors=3)\n\n# Fit and transform\nsdf = imputer.fit(sdf).transform(sdf)\n\n## Categorical col\n# Create dictionary to fill nulls with unknown for those columns\nfill_values = {col_name: 'UNKNOWN' for col_name in cat_cols}\n# Apply fillna to categorical col \nsdf = sdf.fillna(fill_values)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f92abedb-7292-4929-ad0d-91088ea4ffae",
   "metadata": {
    "language": "python",
    "name": "VarianceThreshold"
   },
   "outputs": [],
   "source": "session.use_warehouse(snowpark_opt_warehouse)\n\nfrom snowflake.snowpark import DataFrame\n#from snowflake.snowpark.functions import var_pop\n\n## get all columns with stringType= type\nexcluded = ['MEMBER_ID', 'TARGET','REF_MMYYYY','CAT_1','CAT_2','CAT_3','CAT_4','CAT_5']\nnum_cols = [col for col in sdf.columns if col not in excluded]\n\nsession.use_warehouse('SNOWPARK_OPT_W')\nprint(f'number of features before the variance threshold {len(num_cols)}')\n\n# get the\nvariance_df = sdf.select([F.var_pop(F.col(c)).alias(c) for c in num_cols])\n\nvariance_df = variance_df.to_pandas()\ncols_below_threshold  = variance_df.columns[(variance_df  < 0.1).all()]\nprint( f\" total cols having variance threshold less than 0.1  is {len(cols_below_threshold)}\")\n\nsdf=sdf.drop(*cols_below_threshold )\n\nprint(f'number of features after applying the variance threshold  {len(cols_below_threshold)}')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51a02b5d-815b-4770-96d3-f8a7f0b950c4",
   "metadata": {
    "language": "python",
    "name": "cor_thres"
   },
   "outputs": [],
   "source": "\nfrom snowflake.ml.modeling.metrics.correlation import correlation\n\n\ndef snf_correlation_thresholder(df, features, corr_threshold: float):\n    assert 0 < corr_threshold <= 1, \"Correlation threshold must be in range (0, 1].\"\n    \n    corr_features = set()\n    corr_matrix = correlation(df=sdf)\n\n    # Compute pairwise correlations directly in Snowpark\n    for i in range(len(features)):\n        for j in range(i + 1, len(features)):\n            if (abs(corr_matrix.iloc[i][j])) >=  corr_threshold:\n            #col1, col2 = features[i], features[j]\n            #corr_value = df.select(corr(col(col1), col(col2)).alias('corr')).collect()[0]['CORR']\n            \n           # if corr_value is not None and abs(corr_value) >= corr_threshold:\n                # Mark the second feature for removal to avoid keeping highly correlated pairs\n                #corr_features.add(col2)\n                corr_features.add(features[j])\n    \n    # Drop correlated features if any\n    if corr_features:\n        df = df.drop(*corr_features)\n        \n    return df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3438836-31ba-4ac2-8955-b7b2eecbccf7",
   "metadata": {
    "language": "python",
    "name": "DropCorCols"
   },
   "outputs": [],
   "source": "excluded = ['MEMBER_ID', 'TARGET','REF_MMYYYY','CAT_1','CAT_2','CAT_3','CAT_4','CAT_5']\nnum_cols = [col for col in sdf.columns if col not in excluded]\nprint(f'number of features after applying the variance threshold  {len(sdf.columns)}')\n\nsdf = snf_correlation_thresholder(sdf, num_cols, 0.8)\nprint(f'number of features after applying the variance threshold  {len(sdf.columns)}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b07d37f-8ed7-4b72-86c9-521f107d8ac6",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "--ALTER DATASET Reduced_Dataset DROP VERSION 'V_1';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0e98913-ab6e-4d0c-ad8c-3423c4240cf5",
   "metadata": {
    "language": "python",
    "name": "Dataset"
   },
   "outputs": [],
   "source": "from snowflake.ml import dataset\n# Materialize DataFrame contents into a Dataset\nds = dataset.create_from_dataframe(\n    session,\n    \"Reduced_Dataset\",\n    \"V_1\",\n    input_dataframe=sdf)",
   "execution_count": null
  }
 ]
}